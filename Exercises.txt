Objective: understand the value iteration algorithm by changing planning and problem values.

Syntax: $maze /path/to/problemfile

The default behavior is to parse/read a maze description file, display the maze and then perform value iteration until the maximum update error is satisfied.  The resulting (best) policy is then displayed.

1) The problem file Maze/maze.prob describes a maze with traps.  Try creating new files or changing the size of the grid, the number of traps, the value of p_traps (probability of getting trapped) and the error (used as convergence criteria) and see what happens.

2) The file src/maze.h describes the underlying MDP and contains, among other things, the problem's reward distribution.  Try changing these values, particularly the rewards for steps and traps and see how the policy changes.  If there isn't much punishment, getting trapped may not seem so bad.  If steps are not punished, an agent may be willing to take longer paths.  Rewards ultimately define preferences and affect action selection.

3) For systematic testing the program is set up to use a constant seed for the random number generator.  If multiple mazes are created in sequence, this sequence will be repeated everytime the program is executed.  In order to randomize the mazes every time the program is executed, change the random seed in 'maze.cpp' from 0 to time(NULL).
